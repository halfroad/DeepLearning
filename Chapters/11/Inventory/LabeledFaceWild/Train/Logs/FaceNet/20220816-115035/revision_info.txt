arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/Logs/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 14 --lfw_pairs ../Inventory/Pairs/pairs.txt
--------------------
tensorflow version: 2.9.1
--------------------
git hash: b'95d0717c8180d48bc81df0158ee49e5c2ac71546'
--------------------
b'diff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\nindex eac5a78ef..96050a4c4 100644\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\n@@ -160,7 +160,7 @@ def SwitchEnvironmentVariables():\n      \n      GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\n      \n-     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/Logs/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 14 --lfw_pairs ../Inventory/Pairs/pairs.txt\n      \n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\n      \ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/arguments.txt\ndeleted file mode 100644\nindex 25b86c5c0..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 16\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/events.out.tfevents.1660579885.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/events.out.tfevents.1660579885.JinHui.halfroad.com\ndeleted file mode 100644\nindex 4fc203dbc..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/events.out.tfevents.1660579885.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/revision_info.txt\ndeleted file mode 100644\nindex 6c1fe643d..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-000939/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..3395eeeaf 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,7 +415,7 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n-    assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n+   # assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n     lab_array = np.zeros((nrof_images,))\\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..b088d0270 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,12 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003226/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003226/arguments.txt\ndeleted file mode 100644\nindex 25b86c5c0..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003226/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 16\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003226/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003226/revision_info.txt\ndeleted file mode 100644\nindex 70ec401a5..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003226/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..66b8f0eac 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,10 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    \\n+    print("nrof_image = {}, batch_size = {}, pairs[nrof_skipped_pairs] = {}", nrof_image, batch_size, pairs[nrof_skipped_pairs])\\n+\\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +577,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+        \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/arguments.txt\ndeleted file mode 100644\nindex 25b86c5c0..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 16\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/events.out.tfevents.1660581290.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/events.out.tfevents.1660581290.JinHui.halfroad.com\ndeleted file mode 100644\nindex 6bf0502aa..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/events.out.tfevents.1660581290.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/revision_info.txt\ndeleted file mode 100644\nindex 4a22dd316..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-003258/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..2178c1527 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,10 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    \\n+    print("nrof_image = {}, batch_size = {}, pairs[nrof_skipped_pairs] = {}".format(nrof_image, batch_size, pairs[nrof_skipped_pairs]))\\n+\\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +577,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+        \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004258/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004258/arguments.txt\ndeleted file mode 100644\nindex 25b86c5c0..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004258/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 16\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004258/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004258/revision_info.txt\ndeleted file mode 100644\nindex c1874de62..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004258/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..fd7c391f2 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,9 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    print("nrof_images = {}, batch_size = {} = {}".format(nrof_images, batch_size))\\n+        \\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +576,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+            \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/arguments.txt\ndeleted file mode 100644\nindex 25b86c5c0..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 16\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/events.out.tfevents.1660581971.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/events.out.tfevents.1660581971.JinHui.halfroad.com\ndeleted file mode 100644\nindex 355cdda16..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/events.out.tfevents.1660581971.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/revision_info.txt\ndeleted file mode 100644\nindex 086ae54d4..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-004412/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..01eefdab9 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,9 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    print("nrof_images = {}, batch_size = {}".format(nrof_images, batch_size))\\n+        \\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +576,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+            \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/arguments.txt\ndeleted file mode 100644\nindex e4029b503..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 19\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/events.out.tfevents.1660582671.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/events.out.tfevents.1660582671.JinHui.halfroad.com\ndeleted file mode 100644\nindex fee07da6b..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/events.out.tfevents.1660582671.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/revision_info.txt\ndeleted file mode 100644\nindex e35e22419..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-005558/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 19 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..01eefdab9 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,9 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    print("nrof_images = {}, batch_size = {}".format(nrof_images, batch_size))\\n+        \\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +576,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+            \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/arguments.txt\ndeleted file mode 100644\nindex e4029b503..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 19\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/events.out.tfevents.1660583265.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/events.out.tfevents.1660583265.JinHui.halfroad.com\ndeleted file mode 100644\nindex c69444825..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/events.out.tfevents.1660583265.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/revision_info.txt\ndeleted file mode 100644\nindex e35e22419..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-010706/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 19 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..01eefdab9 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,9 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    print("nrof_images = {}, batch_size = {}".format(nrof_images, batch_size))\\n+        \\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +576,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+            \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/arguments.txt\ndeleted file mode 100644\nindex 80885164c..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 14\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/events.out.tfevents.1660583502.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/events.out.tfevents.1660583502.JinHui.halfroad.com\ndeleted file mode 100644\nindex d6683b708..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/events.out.tfevents.1660583502.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/revision_info.txt\ndeleted file mode 100644\nindex 75f8eaf14..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-011104/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 14 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..f79d83b2a 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..01eefdab9 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,9 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    print("nrof_images = {}, batch_size = {}".format(nrof_images, batch_size))\\n+        \\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +576,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+            \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/arguments.txt\ndeleted file mode 100644\nindex 80885164c..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 14\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/events.out.tfevents.1660584553.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/events.out.tfevents.1660584553.JinHui.halfroad.com\ndeleted file mode 100644\nindex 40a74af74..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/events.out.tfevents.1660584553.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/lfw_result.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/lfw_result.txt\ndeleted file mode 100644\nindex d31aca546..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/lfw_result.txt\n+++ /dev/null\n@@ -1 +0,0 @@\n-0\t0.90000\t0.00000\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/revision_info.txt\ndeleted file mode 100644\nindex e4c19de6c..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 14 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..6d52b8277 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\n@@ -505,14 +506,25 @@ def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_targe\\n     return val_mean, val_std, far_mean\\n \\n \\n+#def calculate_val_far(threshold, dist, actual_issame):\\n+#    predict_issame = np.less(dist, threshold)\\n+#    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\n+#    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n+#    n_same = np.sum(actual_issame)\\n+#    n_diff = np.sum(np.logical_not(actual_issame))\\n+#    val = float(true_accept) / float(n_same)\\n+#    far = float(false_accept) / float(n_diff)\\n+#    return val, far\\n+\\n def calculate_val_far(threshold, dist, actual_issame):\\n     predict_issame = np.less(dist, threshold)\\n     true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\n     false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n     n_same = np.sum(actual_issame)\\n     n_diff = np.sum(np.logical_not(actual_issame))\\n-    val = float(true_accept) / float(n_same)\\n-    far = float(false_accept) / float(n_diff)\\n+    val = float(true_accept) / float(n_same) if float(n_same) > 0 else 1.0\\n+    far = float(false_accept) / float(n_diff) if float(n_diff) > 0 else 0.0\\n+    \\n     return val, far\\n \\n def store_revision_info(src_path, output_dir, arg_string):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..01eefdab9 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -415,6 +415,9 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    print("nrof_images = {}, batch_size = {}".format(nrof_images, batch_size))\\n+        \\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +576,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+            \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/stat.h5 b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/stat.h5\ndeleted file mode 100644\nindex 82daee09f..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-012832/stat.h5 and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/arguments.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/arguments.txt\ndeleted file mode 100644\nindex 80885164c..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/arguments.txt\n+++ /dev/null\n@@ -1,44 +0,0 @@\n-logs_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/\n-models_base_dir: ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/\n-gpu_memory_fraction: 1.0\n-pretrained_model: None\n-data_dir: ../Inventory/CustomizedDatasetsAligned/\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 1\n-batch_size: 90\n-image_size: 160\n-epoch_size: 8\n-embedding_size: 512\n-random_crop: True\n-random_flip: True\n-random_rotate: False\n-use_fixed_image_standardization: True\n-keep_probability: 0.8\n-weight_decay: 0.0005\n-center_loss_factor: 0.0\n-center_loss_alfa: 0.95\n-prelogits_norm_loss_factor: 0.0005\n-prelogits_norm_p: 1.0\n-prelogits_hist_max: 10.0\n-optimizer: ADAM\n-learning_rate: -1.0\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-nrof_preprocess_threads: 4\n-log_histograms: False\n-learning_rate_schedule_file: FaceNet/data/learning_rate_schedule_classifier_casia.txt\n-filter_filename: \n-filter_percentile: 100.0\n-filter_min_nrof_images_per_class: 0\n-validate_every_n_epochs: 5\n-validation_set_split_ratio: 0.05\n-min_nrof_val_images_per_class: 0\n-lfw_pairs: ../Inventory/Pairs/pairs.txt\n-lfw_dir: ../Inventory/CustomizedDatasetsAligned/\n-lfw_batch_size: 14\n-lfw_nrof_folds: 10\n-lfw_distance_metric: 1\n-lfw_use_flipped_images: True\n-lfw_subtract_mean: True\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/events.out.tfevents.1660584863.JinHui.halfroad.com b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/events.out.tfevents.1660584863.JinHui.halfroad.com\ndeleted file mode 100644\nindex 397351fc2..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/events.out.tfevents.1660584863.JinHui.halfroad.com and /dev/null differ\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/lfw_result.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/lfw_result.txt\ndeleted file mode 100644\nindex f2790d64b..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/lfw_result.txt\n+++ /dev/null\n@@ -1 +0,0 @@\n-0\t1.00000\t0.00000\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/revision_info.txt b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/revision_info.txt\ndeleted file mode 100644\nindex b054d458f..000000000\n--- a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 14 --lfw_pairs ../Inventory/Pairs/pairs.txt\n---------------------\n-tensorflow version: 2.9.1\n---------------------\n-git hash: b\'cf08dba481378e34681659a3db6b7a891f714b9c\'\n---------------------\n-b\'diff --git a/Chapters/11/11.1/FaceNet/src/classifier.py b/Chapters/11/11.1/FaceNet/src/classifier.py\\nindex 749db4d6b..0602fe7f3 100644\\n--- a/Chapters/11/11.1/FaceNet/src/classifier.py\\n+++ b/Chapters/11/11.1/FaceNet/src/classifier.py\\n@@ -26,7 +26,7 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import argparse\\n import facenet\\ndiff --git a/Chapters/11/11.1/FaceNet/src/facenet.py b/Chapters/11/11.1/FaceNet/src/facenet.py\\nindex a73e62336..6d52b8277 100644\\n--- a/Chapters/11/11.1/FaceNet/src/facenet.py\\n+++ b/Chapters/11/11.1/FaceNet/src/facenet.py\\n@@ -32,6 +32,7 @@ from subprocess import Popen, PIPE\\n import tensorflow.compat.v1 as tf\\n import numpy as np\\n from scipy import misc\\n+from matplotlib.pyplot import imread\\n from sklearn.model_selection import KFold\\n from scipy import interpolate\\n from tensorflow.python.training import training\\n@@ -244,7 +245,7 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\\n     nrof_samples = len(image_paths)\\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\\n     for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n+        img = imread(image_paths[i])\\n         if img.ndim == 2:\\n             img = to_rgb(img)\\n         if do_prewhiten:\\n@@ -505,14 +506,25 @@ def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_targe\\n     return val_mean, val_std, far_mean\\n \\n \\n+#def calculate_val_far(threshold, dist, actual_issame):\\n+#    predict_issame = np.less(dist, threshold)\\n+#    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\n+#    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n+#    n_same = np.sum(actual_issame)\\n+#    n_diff = np.sum(np.logical_not(actual_issame))\\n+#    val = float(true_accept) / float(n_same)\\n+#    far = float(false_accept) / float(n_diff)\\n+#    return val, far\\n+\\n def calculate_val_far(threshold, dist, actual_issame):\\n     predict_issame = np.less(dist, threshold)\\n     true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\n     false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n     n_same = np.sum(actual_issame)\\n     n_diff = np.sum(np.logical_not(actual_issame))\\n-    val = float(true_accept) / float(n_same)\\n-    far = float(false_accept) / float(n_diff)\\n+    val = float(true_accept) / float(n_same) if float(n_same) > 0 else 1.0\\n+    far = float(false_accept) / float(n_diff) if float(n_diff) > 0 else 0.0\\n+    \\n     return val, far\\n \\n def store_revision_info(src_path, output_dir, arg_string):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\nindex 475e81bb4..9edcea40d 100644\\n--- a/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n+++ b/Chapters/11/11.1/FaceNet/src/models/inception_resnet_v1.py\\n@@ -23,8 +23,8 @@ from __future__ import absolute_import\\n from __future__ import division\\n from __future__ import print_function\\n \\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n+import tensorflow.compat.v1 as tf\\n+import tf_slim as slim\\n \\n # Inception-Resnet-A\\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\ndiff --git a/Chapters/11/11.1/FaceNet/src/train_softmax.py b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\nindex 6b0b28b58..d43705eed 100644\\n--- a/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n+++ b/Chapters/11/11.1/FaceNet/src/train_softmax.py\\n@@ -31,7 +31,7 @@ import os.path\\n import time\\n import sys\\n import random\\n-import tensorflow as tf\\n+import tensorflow.compat.v1 as tf\\n import numpy as np\\n import importlib\\n import argparse\\n@@ -39,7 +39,7 @@ import facenet\\n import lfw\\n import h5py\\n import math\\n-import tensorflow.contrib.slim as slim\\n+import tf_slim as slim\\n from tensorflow.python.ops import data_flow_ops\\n from tensorflow.python.framework import ops\\n from tensorflow.python.ops import array_ops\\n@@ -257,7 +257,7 @@ def main(args):\\n \\n                 print(\\\'Saving statistics\\\')\\n                 with h5py.File(stat_file_name, \\\'w\\\') as f:\\n-                    for key, value in stat.iteritems():\\n+                    for key, value in stat.items():\\n                         f.create_dataset(key, data=value)\\n     \\n     return model_dir\\n@@ -415,6 +415,9 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\\n     sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n     \\n     embedding_size = int(embeddings.get_shape()[1])\\n+    \\n+    print("nrof_images = {}, batch_size = {}".format(nrof_images, batch_size))\\n+        \\n     assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n     nrof_batches = nrof_images // batch_size\\n     emb_array = np.zeros((nrof_images, embedding_size))\\n@@ -573,6 +576,7 @@ def parse_arguments(argv):\\n         help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n     parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n         help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n+            \\n     return parser.parse_args(argv)\\n   \\n \\ndiff --git a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\nindex 3802da147..eac5a78ef 100644\\n--- a/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n+++ b/Chapters/11/11.1/FaceRecognitionWithFaceNet.py\\n@@ -156,6 +156,14 @@ def SwitchEnvironmentVariables():\\n     \\n      python3 FaceNet/src/validate_on_lfw.py ../Inventory/Aligned  /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization --lfw_pairs /Users/jinhui/Projects/DeepLearning/Chapters/11/11.1/FaceNet/data/pairs.txt\\n      \\n+     python3 FaceNet/src/classifier.py TRAIN ../Inventory/CustomizedDatasets /Users/jinhui/Projects/DeepLearning/Chapters/11/Inventory/Models/20180402-114759.pb ../Inventory/Models/OwnedClassifier.pkl --image_size 160\\n+     \\n+     GeneratePairs.py: https://github.com/VictorZhang2014/facenet/blob/master/mydata/generate_pairs.py\\n+     \\n+     python3 FaceNet/src/train_softmax.py --logs_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/ --models_base_dir ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/ --data_dir ../Inventory/CustomizedDatasetsAligned/ --image_size 160 --model_def models.inception_resnet_v1 --lfw_dir ../Inventory/CustomizedDatasetsAligned/ --optimizer ADAM --learning_rate -1 --max_nrof_epochs 1 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file FaceNet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4 --epoch_size 8 --lfw_batch_size 16 --lfw_pairs ../Inventory/Pairs/pairs.txt\\n+     \\n+     python3 FaceNet/src/validate_on_lfw.py ../Inventory/CustomizedDatasetsAligned ../Inventory/LabeledFaceWild/Train/FaceNet/Models/FaceNet/20220816-000939 --lfw_pairs ../Inventory/Pairs/pairs.txt --lfw_batch_size 2 --image_size 160 --distance_metric 1 --use_flipped_images --subtract_mean --use_fixed_image_standardization\\n+     \\n     \\\'\\\'\\\'\\n \\n \\ndiff --git a/Chapters/11/11.1/GeneratePairs.py b/Chapters/11/11.1/GeneratePairs.py\\nindex 7a12eac86..0f917857b 100644\\n--- a/Chapters/11/11.1/GeneratePairs.py\\n+++ b/Chapters/11/11.1/GeneratePairs.py\\n@@ -1,68 +1,79 @@\\n-import sys\\n+#! encoding: utf-8\\n+\\n import os\\n import random\\n-import time\\n-import itertools\\n-import pdb\\n-import argparse\\n-\\n-\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'generate image pairs\\\')\\n-\\n-parser.add_argument(\\\'--data-dir\\\', default=\\\'\\\', help=\\\'\\\')\\n-parser.add_argument(\\\'--outputtxt\\\', default=\\\'\\\', help=\\\'path to save.\\\')\\n-parser.add_argument(\\\'--num-samepairs\\\',default=100)\\n-args = parser.parse_args()\\n-cnt = 0\\n-same_list = []\\n-diff_list = []\\n-list1 = []\\n-list2 = []\\n-folders_1 = os.listdir(args.data_dir)\\n-dst = open(args.outputtxt, \\\'a\\\')\\n-count = 0\\n-dst.writelines(\\\'\\\\n\\\')\\n-\\n-for folder in folders_1:\\n-    sublist = []\\n-    same_list = []\\n-    imgs = os.listdir(os.path.join(args.data_dir, folder))\\n-    for img in imgs:\\n-        img_root_path = os.path.join(args.data_dir, folder, img)\\n-        sublist.append(img_root_path)\\n-        list1.append(img_root_path)\\n-    for item in itertools.combinations(sublist, 2):\\n-        for name in item:\\n-            same_list.append(name)\\n-    if len(same_list) > 10 and len(same_list) < 13:\\n-        for j in range(0, len(same_list), 2):\\n-                if count < int(args.num_samepairs):\\n-                    dst.writelines(same_list[j] + \\\' \\\' + same_list[j+1]+ \\\' \\\' + \\\'1\\\' + \\\'\\\\n\\\')\\n-                    count += 1\\n-    if count >= int(args.num_samepairs):\\n-        break\\n-list2 = list1.copy()\\n-\\n-\\n-\\n-diff = 0\\n-print(count)\\n-\\n-\\n-while True:\\n-    random.seed(time.time() * 100000 % 10000)\\n-    random.shuffle(list2)\\n-    for p in range(0, len(list2) - 1, 2):\\n-        if list2[p] != list2[p + 1]:\\n-            dst.writelines(list2[p] + \\\' \\\' + list2[p + 1] + \\\' \\\' + \\\'0\\\'+ \\\'\\\\n\\\')\\n-            diff += 1\\n-            if diff >= count:\\n-                break\\n-            \\n-    if diff < count:\\n-        \\n-        continue\\n-    else:\\n-        break\\n+\\n+class GeneratePairs:\\n+    """\\n+    Generate the pairs.txt file that is used for training face classifier when calling python `src/train_softmax.py`.\\n+    Or others\\\' python scripts that needs the file of pairs.txt.\\n+\\n+    Doc Reference: http://vis-www.cs.umass.edu/lfw/README.txt\\n+    """\\n+\\n+    def __init__(self, data_dir, pairs_filepath, img_ext):\\n+        """\\n+        Parameter data_dir, is your data directory.\\n+        Parameter pairs_filepath, where is the pairs.txt that belongs to.\\n+        Parameter img_ext, is the image data extension for all of your image data.\\n+        """\\n+        self.data_dir = data_dir\\n+        self.pairs_filepath = pairs_filepath\\n+        self.img_ext = img_ext\\n+\\n+\\n+    def generate(self):\\n+        self._generate_matches_pairs()\\n+        self._generate_mismatches_pairs()\\n+\\n+\\n+    def _generate_matches_pairs(self):\\n+        """\\n+        Generate all matches pairs\\n+        """\\n+        for name in os.listdir(self.data_dir):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            a = []\\n+            for file in os.listdir(self.data_dir + name):\\n+                if file == ".DS_Store":\\n+                    continue\\n+                a.append(file)\\n+\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    temp = random.choice(a).split("_") # This line may vary depending on how your images are named.\\n+                    w = temp[0] + "_" + temp[1]\\n+                    l = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    r = random.choice(a).split("_")[2].lstrip("0").rstrip(self.img_ext)\\n+                    f.write(w + "\\\\t" + l + "\\\\t" + r + "\\\\n")\\n+\\n+\\n+    def _generate_mismatches_pairs(self):\\n+        """\\n+        Generate all mismatches pairs\\n+        """\\n+        for i, name in enumerate(os.listdir(self.data_dir)):\\n+            if name == ".DS_Store":\\n+                continue\\n+\\n+            remaining = os.listdir(self.data_dir)\\n+            remaining = [f_n for f_n in remaining if f_n != ".DS_Store"]\\n+            # del remaining[i] # deletes the file from the list, so that it is not chosen again\\n+            other_dir = random.choice(remaining)\\n+            with open(self.pairs_filepath, "a") as f:\\n+                for i in range(3):\\n+                    file1 = random.choice(os.listdir(self.data_dir + name))\\n+                    file2 = random.choice(os.listdir(self.data_dir + other_dir))\\n+                    f.write(name + "\\\\t" + file1.split("_")[2].lstrip("0").rstrip(self.img_ext) + "\\\\t")\\n+                f.write("\\\\n")\\n+\\n+\\n+if __name__ == \\\'__main__\\\':\\n+    data_dir = "../Inventory/CustomizedDatasets/"\\n+    pairs_filepath = "../Inventory/Pairs/pairs.txt"\\n+    img_ext = ".jpg"\\n+    generatePairs = GeneratePairs(data_dir, pairs_filepath, img_ext)\\n+    generatePairs.generate()\\n+\'\n\\ No newline at end of file\ndiff --git a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/stat.h5 b/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/stat.h5\ndeleted file mode 100644\nindex 2fc4ee64b..000000000\nBinary files a/Chapters/11/Inventory/LabeledFaceWild/Train/FaceNet/20220816-013344/stat.h5 and /dev/null differ'